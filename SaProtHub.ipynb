{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP8eY1iX29_m"
      },
      "source": [
        "# **SaProt Hub: Collaborative Protein Language Modeling with ColabSaProt**\n",
        "\n",
        "This is the Colab version of [SaProt](https://github.com/westlake-repl/SaProt), a pre-trained protein language model designed for various downstream protein tasks. Our aim is to make SaProt more accessible and user-friendly for biologists, enabling effortless model training and knowledge sharing within the scientific community.\n",
        "\n",
        "We hope this platform can contribute to advancing biological research, fostering collaboration, and accelerating discoveries in the field. You can access [our paper](https://www.biorxiv.org/content/10.1101/2023.10.01.560349v2) for further details.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "<img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/Outline.png\" height=\"256\" align=\"center\" style=\"height:256px\">\n",
        "\n",
        "<font color=red>**To view the content, please click on the first option in the left sidebar.**</font>\n"
      ],
      "metadata": {
        "id": "P7wMKdufvvCn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1-yVVht23Sy"
      },
      "source": [
        "# **1: Installation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXlse3LuEVJO",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137671d8-30f9-422b-e476-c71e95003989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.2/279.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.6/419.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installation finished!\n"
          ]
        }
      ],
      "source": [
        "#@title 1.1: Clickt the run button ▶️ to install SaProt\n",
        "\n",
        "#@markdown (Please waiting for 2-8 minutes to install...)\n",
        "\n",
        "!mkdir -p /content/saprot/LMDB\n",
        "!mkdir -p /content/saprot/bin\n",
        "# !mkdir -p /content/saprot/tmp/af2_structures/\n",
        "# !mkdir -p /content/saprot/weights\n",
        "!mkdir -p /content/saprot/output\n",
        "!mkdir -p /content/saprot/adapters/classification\n",
        "!mkdir -p /content/saprot/adapters/regression\n",
        "!mkdir -p /content/saprot/adapters/token_classification\n",
        "!mkdir -p /content/saprot/structures\n",
        "# !mkdir -p /content/saprot/training_monitor\n",
        "\n",
        "################################################################################\n",
        "########################### install saprot #####################################\n",
        "################################################################################\n",
        "# !pip install lmdb\n",
        "# !pip install transformers==4.28.0 --quiet\n",
        "# !pip install loguru --quiet\n",
        "# !pip install multiprocess --quiet\n",
        "\n",
        "!pip install gdown==v4.6.3 --force-reinstall --quiet\n",
        "!gdown https://drive.google.com/drive/folders/1ECKe5clJXs4POlScVggRQDrFo5HJpGBN?usp=drive_link -O /content/saprot/ --folder  --quiet && pip install /content/saprot/ColabSaProtSetup/saprot-0.4.3-py3-none-any.whl --quiet\n",
        "!chmod +x /content/saprot/ColabSaProtSetup/foldseek\n",
        "\n",
        "!rsync -a --remove-source-files /content/saprot/ColabSaProtSetup/upload_files /content/saprot\n",
        "!rsync -a --remove-source-files /content/saprot/ColabSaProtSetup/datasets /content/saprot\n",
        "!mv /content/saprot/ColabSaProtSetup/foldseek /content/saprot/bin/\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################## global ######################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "import ipywidgets\n",
        "from google.colab import widgets\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torch\n",
        "import copy\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "from loguru import logger\n",
        "\n",
        "import yaml\n",
        "import argparse\n",
        "\n",
        "from easydict import EasyDict\n",
        "from datetime import datetime\n",
        "\n",
        "# from saprot.utils.others import setup_seed\n",
        "# from saprot.utils.module_loader import *\n",
        "# from saprot.model.esm.esm_classification_model import EsmClassificationModel\n",
        "# from saprot.model.esm.esm_regression_model import EsmRegressionModel\n",
        "# from saprot.dataset.esm.esm_classification_dataset import EsmClassificationDataset\n",
        "# from saprot.dataset.esm.esm_regression_dataset import EsmRegressionDataset\n",
        "\n",
        "DATASET_HOME = Path('/content/saprot/datasets')\n",
        "ADAPTER_HOME = Path('/content/saprot/adapters')\n",
        "STRUCTURE_HOME = Path(\"/content/saprot/structures\")\n",
        "LMDB_HOME = Path('/content/saprot/LMDB')\n",
        "OUTPUT_HOME = Path('/content/saprot/output')\n",
        "FOLDSEEK_PATH = Path(\"/content/saprot/bin/foldseek\")\n",
        "aa_set = {\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"}\n",
        "foldseek_struc_vocab = \"pynwrqhgdlvtmfsaeikc#\"\n",
        "\n",
        "task_type_dict = {\n",
        "  \"Classify protein sequences (classification)\" : \"classification\",\n",
        "  \"Classify each Amino Acid (token classification), e.g. Binding site detection\" : \"token_classification\",\n",
        "  \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\" : \"regression\",\n",
        "}\n",
        "model_type_dict = {\n",
        "  \"classification\" : \"esm/esm_classification_model\",\n",
        "  \"token_classification\" : \"esm/esm_token_classification_model\",\n",
        "  \"regression\" : \"esm/esm_regression_model\",\n",
        "}\n",
        "dataset_type_dict = {\n",
        "  \"classification\": \"esm/esm_classification_dataset\",\n",
        "  \"token_classification\" : \"esm/esm_token_classification_dataset\",\n",
        "  \"regression\": \"esm/esm_regression_dataset\",\n",
        "}\n",
        "class font:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "    RESET = '\\033[0m'\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################## DATASET #####################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "############################# dataset list #####################################\n",
        "################################################################################\n",
        "def get_datasets_list():\n",
        "    file_list = []\n",
        "    directory_path = DATASET_HOME\n",
        "\n",
        "    for file_path in directory_path.iterdir():\n",
        "        if file_path.is_file():\n",
        "            file_list.append(file_path)\n",
        "\n",
        "    return file_list\n",
        "\n",
        "def show_datasets_info(datasets_list):\n",
        "  grid = widgets.Grid(len(datasets_list)+1, 3, header_row=True, header_column=True)\n",
        "\n",
        "  with grid.output_to(0, 0):\n",
        "    print(\"ID\")\n",
        "\n",
        "  with grid.output_to(0, 1):\n",
        "    print(\"Dataset\")\n",
        "\n",
        "  with grid.output_to(0, 2):\n",
        "    print(\"Dataset Path\")\n",
        "\n",
        "  for i in range(len(datasets_list)):\n",
        "    with grid.output_to(i+1, 0):\n",
        "      print(i)\n",
        "    with grid.output_to(i+1, 1):\n",
        "      print(datasets_list[i].stem)\n",
        "    with grid.output_to(i+1, 2):\n",
        "      print(datasets_list[i])\n",
        "\n",
        "def datasets_dropdown(datasets_list):\n",
        "  dropdown = ipywidgets.Dropdown(\n",
        "      options=[f\"{index}. {file.stem}\" for index, file in enumerate(datasets_list)],\n",
        "      value=None,\n",
        "      # description='Selected:',\n",
        "      disabled=False,)\n",
        "  dropdown.layout.width = \"500px\"\n",
        "  display(dropdown)\n",
        "  return dropdown\n",
        "\n",
        "def select_dataset():\n",
        "  datasets_list = get_datasets_list()\n",
        "  print(font.RED+font.BOLD+\"Existing Datasets:\"+font.RESET)\n",
        "  print(\"=\"*100)\n",
        "  show_datasets_info(datasets_list)\n",
        "  print(\"=\"*100)\n",
        "\n",
        "  return datasets_dropdown(datasets_list)\n",
        "\n",
        "################################################################################\n",
        "############################# adapter list #####################################\n",
        "################################################################################\n",
        "\n",
        "def get_adapters_list():\n",
        "    file_list = []\n",
        "    directory_path = ADAPTER_HOME\n",
        "\n",
        "    for file_path in (directory_path / \"classification\").iterdir():\n",
        "        if file_path.is_dir():\n",
        "            file_list.append(file_path)\n",
        "\n",
        "    for file_path in (directory_path / \"regression\").iterdir():\n",
        "        if file_path.is_dir():\n",
        "            file_list.append(file_path)\n",
        "\n",
        "    for file_path in (directory_path / \"token_classification\").iterdir():\n",
        "        if file_path.is_dir():\n",
        "            file_list.append(file_path)\n",
        "\n",
        "    file_list = [filename for filename in file_list if not filename.stem.startswith('.')]\n",
        "\n",
        "    return file_list\n",
        "\n",
        "def show_adapters_info(adapters_list):\n",
        "  grid = widgets.Grid(len(adapters_list)+1, 3, header_row=True, header_column=True)\n",
        "\n",
        "  with grid.output_to(0, 0):\n",
        "    print(\"ID\")\n",
        "\n",
        "  with grid.output_to(0, 1):\n",
        "    print(\"Local Adapter\")\n",
        "\n",
        "  with grid.output_to(0, 2):\n",
        "    print(\"Adapter Path\")\n",
        "\n",
        "  for i in range(len(adapters_list)):\n",
        "    with grid.output_to(i+1, 0):\n",
        "      print(i)\n",
        "    with grid.output_to(i+1, 1):\n",
        "      print(adapters_list[i].stem)\n",
        "    with grid.output_to(i+1, 2):\n",
        "      print(adapters_list[i])\n",
        "\n",
        "# def adapters_dropdown(adapters_list):\n",
        "#   dropdown = ipywidgets.Dropdown(\n",
        "#       options=[f\"{index}. {file.stem}\" for index, file in enumerate(adapters_list)],\n",
        "#       value=None,\n",
        "#       description='Selected:',\n",
        "#       disabled=False,)\n",
        "#   dropdown.layout.width = \"500px\"\n",
        "#   display(dropdown)\n",
        "\n",
        "#   return dropdown\n",
        "\n",
        "def adapters_combobox(adapters_list):\n",
        "  combobox = ipywidgets.Combobox(\n",
        "    options=[f\"{index}. {adapter_path.stem}\" for index, adapter_path in enumerate(adapters_list)],\n",
        "    value=None,\n",
        "    placeholder='Enter a huggingface repo_id or select an local adapter here',\n",
        "    # description='Selected:',\n",
        "    disabled=False)\n",
        "  combobox.layout.width = '500px'\n",
        "  display(combobox)\n",
        "\n",
        "  return combobox\n",
        "\n",
        "def select_adapter():\n",
        "  adapters_list = get_adapters_list()\n",
        "  print(font.RED+font.BOLD+\"Existing Adapters:\"+font.RESET)\n",
        "  print(\"=\"*100)\n",
        "  show_adapters_info(adapters_list)\n",
        "  print(\"=\"*100)\n",
        "  return adapters_combobox(adapters_list)\n",
        "\n",
        "################################################################################\n",
        "########################### download dataset ###################################\n",
        "################################################################################\n",
        "def download_dataset(task_name):\n",
        "  import gdown\n",
        "  import tarfile\n",
        "\n",
        "  filepath = LMDB_HOME / f\"{task_name}.tar.gz\"\n",
        "  download_links = {\n",
        "    \"ClinVar\" : \"https://drive.google.com/uc?id=1Le6-v8ddXa1eLJZFo7HPij7NhaBmNUbo\",\n",
        "    \"DeepLoc_cls2\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"DeepLoc_cls10\" : \"https://drive.google.com/uc?id=1dGlojkCt1DwUXWiUk4kXRGRNu5sz2uxf\",\n",
        "    \"EC\" : \"https://drive.google.com/uc?id=1VFLFA-jK1tkTZBVbMw8YSsjZqAqlVQVQ\",\n",
        "    \"GO_BP\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_CC\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"GO_MF\" : \"https://drive.google.com/uc?id=1DGiGErWbRnEK8jmE2Jpb996By8KVDBfF\",\n",
        "    \"HumanPPI\" : \"https://drive.google.com/uc?id=1ahgj-IQTtv3Ib5iaiXO_ASh2hskEsvoX\",\n",
        "    \"MetalIonBinding\" : \"https://drive.google.com/uc?id=1rwknPWIHrXKQoiYvgQy4Jd-efspY16x3\",\n",
        "    \"ProteinGym\" : \"https://drive.google.com/uc?id=1L-ODrhfeSjDom-kQ2JNDa2nDEpS8EGfD\",\n",
        "    \"Thermostability\" : \"https://drive.google.com/uc?id=1I9GR1stFDHc8W3FCsiykyrkNprDyUzSz\",\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    gdown.download(download_links[task_name], str(filepath), quiet=False)\n",
        "    with tarfile.open(filepath, 'r:gz') as tar:\n",
        "      tar.extractall(path=str(LMDB_HOME))\n",
        "      print(f\"Extracted: {filepath}\")\n",
        "  except Exception as e:\n",
        "    raise RuntimeError(\"The dataset has not prepared.\")\n",
        "\n",
        "################################################################################\n",
        "############################# upload file ######################################\n",
        "################################################################################\n",
        "def upload_file(upload_path):\n",
        "  import shutil\n",
        "  import os\n",
        "  from pathlib import Path\n",
        "  import sys\n",
        "\n",
        "  upload_path = Path(upload_path)\n",
        "  upload_path.mkdir(parents=True, exist_ok=True)\n",
        "  basepath = Path().resolve()\n",
        "  try:\n",
        "    uploaded = files.upload()\n",
        "    filenames = []\n",
        "    for filename in uploaded.keys():\n",
        "      filenames.append(filename)\n",
        "      shutil.move(basepath / filename, upload_path / filename)\n",
        "    if len(filenames) == 0:\n",
        "      logger.info(\"The uploading process has been interrupted by the user.\")\n",
        "      raise RuntimeError(\"The uploading process has been interrupted by the user.\")\n",
        "  except Exception as e:\n",
        "    logger.error(\"Upload file fail! Please click the button to run again.\")\n",
        "    raise(e)\n",
        "\n",
        "  return upload_path / filenames[0]\n",
        "\n",
        "################################################################################\n",
        "############################ upload dataset ####################################\n",
        "################################################################################\n",
        "def upload_dataset(data_type):\n",
        "  print(font.RED+font.BOLD+\"Please upload the .csv file\"+font.RESET)\n",
        "\n",
        "  upload_path = Path().resolve() / \"saprot\" / \"upload_files\"\n",
        "  dataset_csv_path = upload_file(upload_path)\n",
        "  print(font.RED+font.BOLD+\"Successfully upload your .csv file!\"+font.RESET)\n",
        "  print(\"=\"*100)\n",
        "\n",
        "  saseq_csv_path = DATASET_HOME / f\"[DATASET]{Path(dataset_csv_path).stem}.csv\"\n",
        "  get_SASequence_by_data_type(data_type, dataset_csv_path, saseq_csv_path)\n",
        "  print()\n",
        "  print(\"=\"*100)\n",
        "  print(font.RED+font.BOLD+\"Successfully upload your dataset!\"+font.RESET)\n",
        "\n",
        "  return saseq_csv_path\n",
        "\n",
        "################################################################################\n",
        "########################## Download predicted structures #######################\n",
        "################################################################################\n",
        "def uniprot2pdb(uniprot_ids, nprocess=20):\n",
        "  from saprot.utils.downloader import AlphaDBDownloader\n",
        "\n",
        "  os.makedirs(STRUCTURE_HOME, exist_ok=True)\n",
        "  af2_downloader = AlphaDBDownloader(uniprot_ids, \"pdb\", save_dir=STRUCTURE_HOME, n_process=20)\n",
        "  af2_downloader.run()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############### Form foldseek sequences by multiple processes ##################\n",
        "################################################################################\n",
        "def pdb2sequence(process_id, idx, uniprot_id, writer):\n",
        "  from saprot.utils.foldseek_util import get_struc_seq\n",
        "\n",
        "  try:\n",
        "    pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.pdb\"\n",
        "    if Path(pdb_path).exists:\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "    else:\n",
        "      pdb_path = f\"{STRUCTURE_HOME}/{uniprot_id}.cif\"\n",
        "      seq = get_struc_seq(FOLDSEEK_PATH, pdb_path, [\"A\"], process_id=process_id)[\"A\"][-1]\n",
        "\n",
        "    writer.write(f\"{uniprot_id}\\t{seq}\\n\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error: {uniprot_id}, {e}\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################## Form SA sequences by uniprotis/pdb/cif ######################\n",
        "################################################################################\n",
        "def get_SASequence_by_data_type(data_type, csv_file_path, seq_file_path):\n",
        "  protein_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "  if data_type == \"Structure Aware Sequence\":\n",
        "    protein_df.to_csv(seq_file_path, index=None)\n",
        "    return\n",
        "\n",
        "  if data_type == \"Amino Acid Sequence\":\n",
        "\n",
        "    for index, value in protein_df['Sequence'].items():\n",
        "      sa_seq = ''\n",
        "      for aa in value:\n",
        "        sa_seq += aa + '#'\n",
        "      protein_df.at[index, 'Sequence'] = sa_seq\n",
        "\n",
        "    protein_df.to_csv(seq_file_path, index=None)\n",
        "    return\n",
        "\n",
        "  from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "\n",
        "  if data_type == \"UniProt ID\":\n",
        "    protein_list = protein_df.iloc[:, 0].tolist()\n",
        "    uniprot2pdb(protein_list)\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=1, return_results=True)\n",
        "    outputs = mprs.run()\n",
        "\n",
        "    protein_df['Sequence'] = [output.split(\"\\t\")[1] for output in outputs]\n",
        "    protein_df.to_csv(seq_file_path, index=None)\n",
        "\n",
        "  elif data_type == \"PDB/CIF file\":\n",
        "    # upload and unzip PDB file\n",
        "    print(font.RED+font.BOLD+\"Please upload your .zip file that contains .pdb/.cif files\"+font.RESET)\n",
        "    pdb_zip_path = upload_file(Path(\"/content/saprot/upload_files\"))\n",
        "    if pdb_zip_path.suffix != \".zip\":\n",
        "      logger.error(\"The data type does not match. Please click the run button again to upload a .zip file!\")\n",
        "      raise RuntimeError(\"The data type does not match.\")\n",
        "    print(font.RED+font.BOLD+\"Successfully upload your .zip file!\"+font.RESET)\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(pdb_zip_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(STRUCTURE_HOME)\n",
        "      protein_list = [Path(file).stem for file in zip_ref.namelist()]\n",
        "\n",
        "    from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=1, return_results=True)\n",
        "    seqs = mprs.run()\n",
        "\n",
        "    seqs_dict = {}\n",
        "    for seq in seqs:\n",
        "      key, value = seq.split('\\t')\n",
        "      seqs_dict[key] = value\n",
        "\n",
        "    for index, value in protein_df['Sequence'].items():\n",
        "      protein_df.at[index, 'Sequence'] = seqs_dict[value.split(\".\")[0]]\n",
        "\n",
        "    protein_df.to_csv(seq_file_path, index=None)\n",
        "\n",
        "  else:\n",
        "    raise RuntimeError(\"Wrong data type!\")\n",
        "\n",
        "################################################################################\n",
        "############### Form Single SA sequences by uniprotis/pdb/cif ##################\n",
        "################################################################################\n",
        "def get_single_SASequence_by_data_type(data_type, raw_data):\n",
        "  from saprot.utils.mpr import MultipleProcessRunnerSimplifier\n",
        "  if data_type == \"Amino Acid Sequence\":\n",
        "    sa_seq = ''\n",
        "    for aa in raw_data:\n",
        "        sa_seq += aa + '#'\n",
        "  elif data_type == \"UniProt ID\":\n",
        "    protein_list = [raw_data]\n",
        "    uniprot2pdb(protein_list)\n",
        "    mprs = MultipleProcessRunnerSimplifier(protein_list, pdb2sequence, n_process=1, return_results=True)\n",
        "    seqs = mprs.run()\n",
        "    sa_seq = seqs[0].split('\\t')[1]\n",
        "\n",
        "  elif data_type == \"PDB/CIF file\":\n",
        "    # # upload and unzip PDB file\n",
        "    # print(font.RED+font.BOLD+\"Please upload a .pdb/.cif file\"+font.RESET)\n",
        "    # pdb_file_path = upload_file(Path(\"/content/saprot/tmp/af2_structures/\"))\n",
        "    # print(font.RED+font.BOLD+\"Successfully upload your .pdb/.cif file!\"+font.RESET)\n",
        "    # print(\"=\"*100)\n",
        "\n",
        "    # protein_list = [Path(pdb_file_path).stem]\n",
        "\n",
        "    mprs = MultipleProcessRunnerSimplifier(raw_data, pdb2sequence, n_process=1, return_results=True)\n",
        "    seqs = mprs.run()\n",
        "    sa_seq = seqs[0].split('\\t')[1]\n",
        "\n",
        "  else:\n",
        "    raise RuntimeError(\"wrong data type!\")\n",
        "\n",
        "  return sa_seq\n",
        "\n",
        "  # print()\n",
        "  # print(\"=\"*100)\n",
        "  # print(font.RED + font.BOLD + \"The Structure-Aware Sequence is here, double click to select and copy it:\" + font.RESET)\n",
        "  # print(seqs[0].split('\\t')[1])\n",
        "\n",
        "# ################################################################################\n",
        "# ################################################################################\n",
        "# ################################ FINETUNE ######################################\n",
        "# ################################################################################\n",
        "# ################################################################################\n",
        "\n",
        "\n",
        "# ################################################################################\n",
        "# ################################ load model ####################################\n",
        "# ################################################################################\n",
        "# def load_model(config):\n",
        "#     model_config = copy.deepcopy(config)\n",
        "#     model_type = model_config.pop(\"model_py_path\")\n",
        "#     kwargs = model_config.pop('kwargs')\n",
        "#     model_config.update(kwargs)\n",
        "#     print(model_config)\n",
        "#     if model_type == \"esm/esm_classification_model\":\n",
        "#       return EsmClassificationModel(**model_config)\n",
        "#     if model_type == \"esm/esm_regression_model\":\n",
        "#       if 'num_labels' in model_config.keys():\n",
        "#         model_config.pop(\"num_labels\")\n",
        "#       return EsmRegressionModel(**model_config)\n",
        "\n",
        "# ################################################################################\n",
        "# ################################ load dataset ##################################\n",
        "# ################################################################################\n",
        "# def load_dataset(config):\n",
        "#     dataset_config = copy.deepcopy(config)\n",
        "#     dataset_type = dataset_config.pop(\"dataset_py_path\")\n",
        "#     kwargs = dataset_config.pop('kwargs')\n",
        "#     dataset_config.update(kwargs)\n",
        "\n",
        "#     if dataset_type == \"esm/esm_classification_dataset\":\n",
        "#       return EsmClassificationDataset(**dataset_config)\n",
        "#     if dataset_type == \"esm/esm_regression_dataset\":\n",
        "#       return EsmRegressionDataset(**dataset_config)\n",
        "\n",
        "# ################################################################################\n",
        "# ################################## finetune ####################################\n",
        "# ################################################################################\n",
        "# def finetune(config, run_mode):\n",
        "#     if config.setting.seed:\n",
        "#         setup_seed(config.setting.seed)\n",
        "\n",
        "#     for k, v in config.setting.os_environ.items():\n",
        "#         if v is not None and k not in os.environ:\n",
        "#             os.environ[k] = str(v)\n",
        "\n",
        "#         elif k in os.environ:\n",
        "#             config.setting.os_environ[k] = os.environ[k]\n",
        "\n",
        "#     if config.setting.os_environ.NODE_RANK != 0:\n",
        "#         config.Trainer.logger = False\n",
        "\n",
        "#     ############################################################################\n",
        "#     model = load_model(config.model)\n",
        "#     data_module = load_dataset(config.dataset)\n",
        "#     trainer = load_trainer(config)\n",
        "\n",
        "\n",
        "#     trainer.fit(model=model, datamodule=data_module)\n",
        "\n",
        "#     ############################################################################\n",
        "#     if model.save_path is not None:\n",
        "#         if config.model.kwargs.get(\"use_lora\", False):\n",
        "#             # Load LoRA model\n",
        "#             config.model.kwargs.lora_config_path = model.save_path\n",
        "#             config.model.kwargs.lora_inference = True\n",
        "#             model = load_model(config.model)\n",
        "#         else:\n",
        "#             model.load_checkpoint(model.save_path, load_prev_scheduler=model.load_prev_scheduler)\n",
        "\n",
        "\n",
        "#     trainer.test(model=model, datamodule=data_module)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################ INFERENCE #####################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "################################ zeroshot func #################################\n",
        "################################################################################\n",
        "# def zeroshot(mutation_task, seq, mut_info):\n",
        "#   with torch.no_grad():\n",
        "#     # single-site / multi-site\n",
        "#     if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "#       tokens = tokenizer.tokenize(seq)\n",
        "#       for single in mut_info.split(\":\"):\n",
        "#           pos = int(single[1:-1])\n",
        "#           tokens[pos - 1] = \"#\" + tokens[pos - 1][-1]\n",
        "\n",
        "#       mask_seq = \" \".join(tokens)\n",
        "#       inputs = tokenizer(mask_seq, return_tensors=\"pt\")\n",
        "#       inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "#       outputs = model(**inputs)\n",
        "#       logits = outputs.logits\n",
        "#       probs = logits.softmax(dim=-1)\n",
        "\n",
        "#       score = 0\n",
        "#       for single in mut_info.split(\":\"):\n",
        "#           ori_aa, pos, mut_aa = single[0], int(single[1:-1]), single[-1]\n",
        "#           ori_st = tokenizer.get_vocab()[ori_aa + foldseek_struc_vocab[0]]\n",
        "#           mut_st = tokenizer.get_vocab()[mut_aa + foldseek_struc_vocab[0]]\n",
        "\n",
        "#           ori_prob = probs[0, pos, ori_st: ori_st + len(foldseek_struc_vocab)].sum()\n",
        "#           mut_prob = probs[0, pos, mut_st: mut_st + len(foldseek_struc_vocab)].sum()\n",
        "\n",
        "#           score += torch.log(mut_prob / ori_prob)\n",
        "#       # print(f\"The score of mutation {mut_info} is {font.RED}{score.item()}{font.RESET}\")\n",
        "\n",
        "#       return score.item()\n",
        "\n",
        "#     # Saturation\n",
        "#     if mutation_task == \"Saturation mutagenesis\":\n",
        "#       scores = []\n",
        "\n",
        "#       ori_seq = [seq[i:i+2] for i in range(0, len(seq), 2)]\n",
        "\n",
        "#       for pos in tqdm(range(1, len(ori_seq)+1)):\n",
        "#         mask_seq = ori_seq.copy()\n",
        "#         mask_seq[pos-1] = \"#\" + ori_seq[pos-1][-1]\n",
        "#         mask_seqs = []\n",
        "#         mask_seqs.append(\" \".join(mask_seq))\n",
        "\n",
        "#         mask_inputs = tokenizer.batch_encode_plus(mask_seqs, return_tensors=\"pt\", padding=True)\n",
        "#         mask_inputs = {k: v.to(device) for k, v in mask_inputs.items()}\n",
        "#         mask_outputs = model(**mask_inputs)\n",
        "#         mask_probs = mask_outputs['logits'].softmax(dim=-1)\n",
        "\n",
        "#         ori_aa = ori_seq[pos-1][0]\n",
        "#         ori_st = tokenizer.get_vocab()[ori_aa + foldseek_struc_vocab[0]]\n",
        "#         ori_prob = mask_probs[0, pos, ori_st: ori_st + len(foldseek_struc_vocab)].sum()\n",
        "\n",
        "#         for mut_aa in aa_set:\n",
        "#           pred = 0\n",
        "#           mut_st = tokenizer.get_vocab()[mut_aa + foldseek_struc_vocab[0]]\n",
        "#           mut_prob = mask_probs[0, pos, mut_st: mut_st + len(foldseek_struc_vocab)].sum()\n",
        "#           pred += torch.log(mut_prob / ori_prob)\n",
        "#           # print(f\"The score of mutation {ori_aa}{pos}{mut_aa} is {font.RED}{pred.item()}{font.RESET}\")\n",
        "#           scores.append(pred.item())\n",
        "\n",
        "#       return scores\n",
        "\n",
        "# ################################################################################\n",
        "# ###################### load model for inference ################################\n",
        "# ################################################################################\n",
        "# def load_model_inference(model_path):\n",
        "#   from transformers import EsmTokenizer, EsmForMaskedLM\n",
        "#   # from saprot.utils.constants import aa_set， foldseek_struc_vocab\n",
        "\n",
        "#   # model_path = \"westlake-repl/SaProt_35M_AF2\"\n",
        "#   tokenizer = EsmTokenizer.from_pretrained(model_path)\n",
        "#   model = EsmForMaskedLM.from_pretrained(model_path)\n",
        "\n",
        "#   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#   model.to(device)\n",
        "\n",
        "print(\"Installation finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brquYlSy9hXl"
      },
      "source": [
        "# **2: Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VROv7yJXTphe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 2.1: Upload your protein sequences dataset\n",
        "\n",
        "# #@markdown <font color=\"red\">Before clicking the run button to upload your dataset, please carefully review this section to learn about the format of data you should prepare.</font>\n",
        "\n",
        "# #@markdown | Sequence | label | stage|\n",
        "# #@markdown | --- | --- | --- |\n",
        "# #@markdown | (depends on data type) | (a number) | (train/valid/test) |\n",
        "#\n",
        "# #@markdown <br>\n",
        "#\n",
        "# #@markdown #### Here are the meanings of these three columns:\n",
        "# #@markdown - `Sequence`: The content of this column dependes on the type of data you have:\n",
        "# #@markdown  - For `Structure Aware Sequence`: The \"Sequence\" column should contain **SA(Structure-Aware) sequence**\n",
        "# #@markdown  - For `AA Sequence`: The \"Sequence\" column should contain **Amino Acid Sequence**\n",
        "# #@markdown  - For `UniProt ID`: The \"Sequence\" column should contain **UniProt ID**\n",
        "# #@markdown  - For `PDB/CIF file`: The \"Sequence\" column should contain **the filenames of your .pdb/.cif file** (<font color = \"red\">**Note that**</font>: If your data type is 'PDB/CIF file', you need to upload an additional .zip file containing your .pdb/.cif file after uploading the .csv file)\n",
        "# #@markdown - `label`:\n",
        "# #@markdown  - For **classification task**, the values in this column should represent the index of your categories (integers ranging from zero to the number of categories).\n",
        "# #@markdown  - For **token classification task**, the values in this column should represent a list of category index for each amino acid.\n",
        "# #@markdown  - For **regression task**, the values in this column should be numerical.\n",
        "#\n",
        "#\n",
        "# #@markdown <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/LabelFormat.png\" height=\"400\" align=\"center\" style=\"height:256px\">\n",
        "#\n",
        "# #@markdown - `stage`: The values in this column(**train/valid/test**) determine whether the sample is used for training, validation, or testing. (<font color = \"red\">**Note that**</font>: Ensure that your dataset includes samples of all three types(train/valid/test).)\n",
        "# # The dataset uploaded by the user must include samples for validation purposes. The model saves checkpoints based on validation metrics. If the validation set is empty, the checkpoints will not be saved.\n",
        "# #@markdown <br>\n",
        "#\n",
        "# # #@markdown ###  The following illustration displays the specific file format and its contents for three data types：\n",
        "#\n",
        "# #@markdown <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/DatasetFormat.png\" height=\"256\" align=\"center\" style=\"height:256px\">\n",
        "# #@markdown  <font color=\"red\">**Note that**</font>: You can find some examples at /content/saprot/upload_files. Download them to review the format and upload them for a try.\n",
        "#\n",
        "# #@markdown <br>\n",
        "\n",
        "\n",
        "#@markdown **Before uploading your dataset, please carefully review this section to understand the required data format.**\n",
        "#@markdown\n",
        "#@markdown ### You need to upload **a .csv file** as your dataset, where the columns should be named as `Sequence`, `label`, and `stage`.\n",
        "##@markdown ### You need to upload a dataset file in the .csv format. Ensure that the columns are named as `Sequence`, `label`, and `stage`.\n",
        "#@markdown\n",
        "##@markdown | Sequence | label | stage|\n",
        "##@markdown | --- | --- | --- |\n",
        "##@markdown | (depends on data type) | (a number) | (train/valid/test) |\n",
        "##@markdown\n",
        "##@markdown #### Explanation of Columns:\n",
        "#@markdown - `Sequence`: The content of this column depends on your **data type**:\n",
        "#@markdown   - For `Structure Aware Sequence`: Provide the \"Sequence\" column with SA (Structure-Aware) sequences.\n",
        "#@markdown   - For `AA Sequence`: The \"Sequence\" column should contain Amino Acid Sequences.\n",
        "#@markdown   - For `UniProt ID`: Input UniProt IDs into the \"Sequence\" column.\n",
        "#@markdown   - For `PDB/CIF file`: Enter filenames of your .pdb/.cif files into the \"Sequence\" column. (Note: For this data type, upload an additional .zip file containing your .pdb/.cif files after uploading the .csv file)\n",
        "#@markdown - `label`: The content of this column depends on your **task type**:\n",
        "#@markdown   - For `classification tasks`: Use integers ranging from zero to the number of categories to represent the categories in this column.\n",
        "#@markdown   - For `token classification tasks`: Provide a list of category indices for each amino acid in this column.\n",
        "#@markdown   - For `regression tasks`: Input numerical values into this column.\n",
        "#@markdown\n",
        "#@markdown\n",
        "#@markdown - `stage`: This column should indicate whether the sample is for training, validation, or testing (train/valid/test). Ensure your dataset includes samples for all three stages.\n",
        "#@markdown\n",
        "# #@markdown ### The illustration below shows the specific file format and contents for three data types：\n",
        "#@markdown\n",
        "\n",
        "#@markdown <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/DatasetFormat.png\" height=\"400\" width=\"700px\" align=\"left\">\n",
        "#@markdown <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/LabelFormat.png\" height=\"400\" width=\"550px\" align=\"left\">\n",
        "\n",
        "#@markdown\n",
        "#@markdown **Note:** Examples are available at /content/saprot/upload_files. Download to review their format, and then upload them for a trial.\n",
        "#@markdown\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################ input #########################################\n",
        "################################################################################\n",
        "data_type = \"Amino Acid Sequence\" #@param [\"Structure Aware Sequence\", \"Amino Acid Sequence\", \"UniProt ID\", \"PDB/CIF file\"]\n",
        "# use_for = \"Training (with Label)\" # @param [\"Training (with Label)\", \"Prediction (without Lable)\"]\n",
        "# task_objective = \"Classify protein sequences (classification)\" # @param [\"Classify protein sequences (classification)\", \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\", \"Classify each Amino Acid (token classification), e.g. Binding site detection\"]\n",
        "# automatically_split_dataset = False # @param {type:\"boolean\"}\n",
        "# check the format of .csv file\n",
        "# print(font.RED+font.BOLD+f\"Data Type: {data_type}\"+font.RESET)\n",
        "# dataset_type = \"Train for classification task (with label)\" # @param [\"Train for classification task (with label)\", \"Train for regression task (with label)\", \"Train for token_classification task (with label)\", \"Prediction for all three tasks (without label)\"]\n",
        "\n",
        "################################################################################\n",
        "############################ upload dataset ####################################\n",
        "################################################################################\n",
        "\n",
        "saseq_csv_path = upload_dataset(data_type)\n",
        "print(font.RED+font.BOLD +f\"Dataset: \\\"{saseq_csv_path}\\\" has been saved to your local computer.\" + font.RESET )\n",
        "\n",
        "# files.download(saseq_csv_path)\n",
        "\n",
        "################################################################################\n",
        "############################## existing dataset ################################\n",
        "################################################################################\n",
        "\n",
        "datasets_list = get_datasets_list()\n",
        "show_datasets_info(datasets_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB-LCoh1VqQu"
      },
      "source": [
        "# **3: Train and Share your PLM**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.1: Task Config\n",
        "##@markdown Complete some task configs and run this cell to Finetune SaProt on your dataset. <br>\n",
        "\n",
        "\n",
        "##@markdown <br>\n",
        "\n",
        "################################################################################\n",
        "from pathlib import Path\n",
        "from easydict import EasyDict\n",
        "import copy\n",
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "\n",
        "################################################################################\n",
        "############################### custom config ##################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "##@markdown ### 1. Enter your task name\n",
        "task_name = \"demo_cls2\" # @param {type:\"string\"}\n",
        "# model_save_path = Path(f\"/content/saprot/weights/{task_name}\")\n",
        "\n",
        "##@markdown ### 2. Select your task type\n",
        "task_objective = \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\" # @param [\"Classify protein sequences (classification)\", \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\", \"Classify each Amino Acid (token classification), e.g. Binding site detection\"]\n",
        "task_type = task_type_dict[task_objective]\n",
        "# num_of_categories = 10 # @param {type:\"number\"}\n",
        "# #@markdown <font face=\"Consolas\" size=2 color='gray'>(Ignoring `num_of_categories` if predicting a value)\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification']:\n",
        "\n",
        "  print(font.RED+font.BOLD+'Enter the number of category in your training dataset here:'+font.RESET)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              max=1000000,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n",
        "  print(font.RED+font.BOLD+'It\\'s normal not to receive feedback once inputting is finished. Let\\'s move on to the next step.'+font.RESET)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iV0DEnU_nwS1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTIPt3uUdPHJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 3.2: Select Dataset\n",
        "\n",
        "\n",
        "dataset_dropdown = select_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.3: Select Model\n",
        "\n",
        "# #@markdown We utilize **LoRA** (A Parameter-Efficient Fine-Tuning Technique), which allows us to store model weights into an small adapter without adjusting the original model weights during training.\n",
        "# #@markdown\n",
        "\n",
        "# #@markdown After training, you can obtain an adapter for your task.\n",
        "\n",
        "#@markdown We use Parameter-Efficient Fine-Tuning Technique for model training. It enables us to store model weights in a small **adapter** without changing the original model weights during training. After training, you can get an adapter specific to your task.\n",
        "# #@markdown As we use Parameter-Efficient Fine-Tuning Technique, which allows us to store model weights into an small adapter without adjusting the original model weights during training, it's necessary to specify both the original model and adapter for prediction.\n",
        "#@markdown\n",
        "#@markdown 1. Select a **base model** from the dropdown box `model_path` below.\n",
        "#@markdown\n",
        "#@markdown 2. If you want to **train on existing adapters**, check the box `use_adapter` below. By running this cell, you will see an **adapter combobox**. We provide two ways to select your adapter:\n",
        "#@markdown  - Select a **local adapters** from the combobox.\n",
        "#@markdown   - Enter a **huggingface repository name** to the combobox. (e.g. \"SaProtAdapters/DeepLoc_cls10_35M\")\n",
        "#@markdown\n",
        "#@markdown You can also find some officical adapters in [here](https://huggingface.co/SaProtAdapters)\n",
        "model_path = \"westlake-repl/SaProt_35M_AF2\" # @param [\"westlake-repl/SaProt_35M_AF2\", \"westlake-repl/SaProt_650M_AF2\"]\n",
        "print(font.RED+font.BOLD+f\"Model: {model_path}\"+font.RESET)\n",
        "use_adapter = False # @param {type:\"boolean\"}\n",
        "\n",
        "if use_adapter:\n",
        "  adapter_combobox = select_adapter()\n",
        "\n",
        "# if use_adapter:\n",
        "#   print(font.RED+font.BOLD+f\"Loaded Adapter: {adapter_combobox.value}\"+font.RESET)\n"
      ],
      "metadata": {
        "id": "TjchiQ_gn5K3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.4: Train your Model\n",
        "\n",
        "################################################################################\n",
        "############################## advance config ##################################\n",
        "################################################################################\n",
        "\n",
        "batch_size = 4 # @param [\"1\", \"2\", \"4\", \"8\"] {type:\"raw\", allow-input: true}\n",
        "max_epochs = 20 # @param [\"10\", \"20\", \"50\"] {type:\"raw\", allow-input: true}\n",
        "learning_rate = 1.0e-3 # @param [\"1.0e-3\", \"5.0e-4\", \"1.0e-4\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "limit_train_batches=1.0\n",
        "limit_val_batches=1.0\n",
        "limit_test_batches=1.0\n",
        "\n",
        "val_check_interval=0.5\n",
        "\n",
        "use_lora = True\n",
        "num_workers = 2\n",
        "\n",
        "################################################################################\n",
        "############################## advance config ##################################\n",
        "################################################################################\n",
        "\n",
        "#@markdown - <font face=\"Consolas\" size=2 color='gray'> `batch_size` depends on the number of training samples during model training. We recommend using the default value of 2 for GPU T4.\n",
        "\n",
        "# #@markdown |  Recommended batch size   | T4  |  A100   |\n",
        "# #@markdown | ---                       | --- |  ---    |\n",
        "# #@markdown | SaProt_35M_AF2            |  4  |    16   |\n",
        "# #@markdown | SaProt_650M_AF2           |  -  |    8    |\n",
        "\n",
        "\n",
        "#@markdown - <font face=\"Consolas\" size=2 color='gray'>`max_epochs` refers to the maximum number of complete passes through the entire dataset during the training process.\n",
        "#@markdown You can adjust `max_epochs` to control training duration. (Note that the max running time of colab is 12hrs for unsubscribed user or 24hrs for colab pro+ user) <br>\n",
        "#@markdown\n",
        "\n",
        "# download_adapter_to_your_computer = True #@param {type:\"boolean\"}\n",
        "download_adapter_to_your_computer = True\n",
        "#@markdown - <font face=\"Consolas\" size=2 color='gray'>`learning_rate` affects the convergence speed of the model.\n",
        "#@markdown Through experimentation, we have found that `1.0e-3` is a good default value for model `SaProt_35M_AF2`.\n",
        "\n",
        "################################################################################\n",
        "################################# DATASET ######################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.utils.construct_lmdb import construct_lmdb\n",
        "dataset_path = DATASET_HOME / f\"{dataset_dropdown.value.split('. ')[1]}.csv\"\n",
        "construct_lmdb(dataset_path, LMDB_HOME, task_name, task_type)\n",
        "dataset_task = LMDB_HOME / task_name\n",
        "# dataset_task = Path('/content/saprot/SaProtHub/Thermostability_StructureSimilarity_70')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################## CONFIG ##########################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.config.config_dict import Default_config\n",
        "config = copy.deepcopy(Default_config)\n",
        "\n",
        "################################################################################\n",
        "config.setting.run_mode = \"train\"\n",
        "\n",
        "if task_type in [\"classification\", \"token_classification\"]:\n",
        "  config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "config.model.model_py_path = model_type_dict[task_type]\n",
        "config.model.save_path = str(ADAPTER_HOME / f\"{task_type}\" / f\"{task_name}\")\n",
        "config.model.kwargs.config_path = model_path\n",
        "\n",
        "config.dataset.dataset_py_path = dataset_type_dict[task_type]\n",
        "config.dataset.train_lmdb = str(dataset_task / \"train\")\n",
        "config.dataset.valid_lmdb = str(dataset_task / \"valid\")\n",
        "config.dataset.test_lmdb = str(dataset_task / \"test\")\n",
        "config.dataset.kwargs.tokenizer = model_path\n",
        "\n",
        "config.Trainer.accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "\n",
        "# epoch, batch size, num_workers\n",
        "config.Trainer.max_epochs = max_epochs\n",
        "\n",
        "config.dataset.dataloader_kwargs.batch_size = batch_size\n",
        "config.Trainer.accumulate_grad_batches= int(64 / batch_size)\n",
        "\n",
        "config.dataset.dataloader_kwargs.num_workers = num_workers\n",
        "\n",
        "# config.dataset.kwargs.mask_struc_ratio= 1.0\n",
        "\n",
        "# learning rate\n",
        "config.model.lr_scheduler_kwargs.init_lr = learning_rate\n",
        "\n",
        "# lora\n",
        "config.model.kwargs.use_lora = use_lora\n",
        "\n",
        "if use_adapter:\n",
        "  if \". \" in adapter_combobox.value:\n",
        "    adapter_path = ADAPTER_HOME / task_type / adapter_combobox.value.split('. ')[1]\n",
        "  else:\n",
        "    adapter_path = adapter_combobox.value\n",
        "  config.model.kwargs.lora_config_path = adapter_path\n",
        "else:\n",
        "  config.model.kwargs.lora_config_path = None\n",
        "\n",
        "if config.setting.run_mode == 'train':\n",
        "  config.model.kwargs.lora_inference = False\n",
        "if config.setting.run_mode == 'test':\n",
        "  config.model.kwargs.lora_inference = True\n",
        "\n",
        "# trainer\n",
        "config.Trainer.limit_train_batches=limit_train_batches\n",
        "config.Trainer.limit_val_batches=limit_val_batches\n",
        "config.Trainer.limit_test_batches=limit_test_batches\n",
        "config.Trainer.val_check_interval=val_check_interval\n",
        "\n",
        "# strategy\n",
        "strategy = {\n",
        "    # - deepspeed\n",
        "    # 'class': 'DeepSpeedStrategy',\n",
        "    # 'stage': 2\n",
        "\n",
        "    # - None\n",
        "    # 'class': None,\n",
        "\n",
        "    # - DP\n",
        "    # 'class': 'DataParallelStrategy',\n",
        "\n",
        "    # - DDP\n",
        "    # 'class': 'DDPStrategy',\n",
        "    # 'find_unused_parameter': True\n",
        "}\n",
        "config.Trainer.strategy = strategy\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################## Run the task ####################################\n",
        "################################################################################\n",
        "\n",
        "print(font.RED+font.BOLD+f\"Training task type: {task_type}\"+font.RESET)\n",
        "print(font.RED+font.BOLD+f\"Dataset: {dataset_task}: {dataset_path}\"+font.RESET)\n",
        "print(font.RED+font.BOLD+f\"Model: {config.model.kwargs.config_path}\"+font.RESET)\n",
        "if use_adapter:\n",
        "  print(font.RED+font.BOLD+f\"Loaded Adapter: {config.model.kwargs.lora_config_path}\"+font.RESET)\n",
        "\n",
        "from saprot.scripts.training import finetune\n",
        "print(config)\n",
        "finetune(config)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "############################## Save the adapter ################################\n",
        "################################################################################\n",
        "\n",
        "print(font.RED+font.BOLD)\n",
        "print(f\"Adapter is saved to \\\"{config.model.save_path}\\\" on colab Sever\")\n",
        "print(font.RESET)\n",
        "\n",
        "if download_adapter_to_your_computer:\n",
        "  adapter_zip = Path(config.model.save_path) / f\"{task_name}.adapter.zip\"\n",
        "  !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"README.md\"\n",
        "  # with zipfile.ZipFile(adapter_zip, 'w') as zipf:\n",
        "  #   zip_files = [str(file_path) for file_path in Path(config.model.save_path).glob(\"*\")]\n",
        "  #   print(zip_files)\n",
        "  #   for file in zip_files:\n",
        "  #     zipf.write(file, Path(file).name)\n",
        "\n",
        "  print(\"Downloading adapter to your local computer\")\n",
        "  files.download(adapter_zip)"
      ],
      "metadata": {
        "id": "O5ATBdy8oFjO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC9lTugHTqgR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **3.5: Login HuggingFace**\n",
        "################################################################################\n",
        "###################### Login HuggingFace #######################################\n",
        "################################################################################\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZpgTWC0yNF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLfdY_8cv1oq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **3.6: Upload Model**\n",
        "\n",
        "#@markdown Your Huggingface adapter repository names follow the format `<username>/<task_name>`.\n",
        "################################################################################\n",
        "########################## Upload Model  #######################################\n",
        "################################################################################\n",
        "from huggingface_hub import HfApi, Repository, ModelFilter\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "user = api.whoami()\n",
        "\n",
        "repo_name = user['name'] + '/' + task_name\n",
        "repo_list = []\n",
        "for repo in api.list_models(filter=ModelFilter(author=user['name'])):\n",
        "  repo_list.append(repo.id)\n",
        "if repo_name not in repo_list:\n",
        "  api.create_repo(repo_name, private=False)\n",
        "\n",
        "local_dir = Path(\"/content/saprot/model_to_push\") / repo_name\n",
        "local_dir.mkdir(parents=True, exist_ok=True)\n",
        "repo = Repository(local_dir=local_dir, clone_from=repo_name)\n",
        "command = f\"cp {config.model.save_path}/* {local_dir}/\"\n",
        "subprocess.run(command, shell=True)\n",
        "\n",
        "repo.push_to_hub(commit_message=\"Upload adapter model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbMkwuRtm9W9"
      },
      "source": [
        "# **4: Predict with PLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1: Classification&Regression"
      ],
      "metadata": {
        "id": "O6h80VBA_yTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.1.1: Task Config\n",
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "import datetime\n",
        "\n",
        "################################################################################\n",
        "################################# task config ##################################\n",
        "################################################################################\n",
        "\n",
        "# # @markdown Please ensure that the selected task type aligns with the training task type of the model you intend to utilize.\n",
        "\n",
        "## @markdown If you are conducting inference on a classification task, please ensure that the `num_of_category` matches the number of categories in the training dataset. Otherwise, you do not need to assign `num_of_category`.\n",
        "task_objective = \"Classify protein sequences (classification)\" # @param [\"Classify protein sequences (classification)\", \"Predict protein fitness (regression), e.g. Predict the Thermostability of a protein\", \"Classify each Amino Acid (token classification), e.g. Binding site detection\"]\n",
        "\n",
        "task_type = task_type_dict[task_objective]\n",
        "# num_of_categories = 10 #@param {type:\"integer\"}\n",
        "\n",
        "if task_type in [\"classification\", 'token_classification']:\n",
        "\n",
        "  print(font.RED+font.BOLD+'Enter the number of category in your training dataset here:'+font.RESET)\n",
        "  num_of_categories = ipywidgets.BoundedIntText(\n",
        "                                              # value=7,\n",
        "                                              min=2,\n",
        "                                              # max=10,\n",
        "                                              step=1,\n",
        "                                              # description='num_of_category: \\n',\n",
        "                                              disabled=False)\n",
        "  num_of_categories.layout.width = \"100px\"\n",
        "  display(num_of_categories)\n"
      ],
      "metadata": {
        "id": "vbWMXqMP1ASy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiKYROiqyI2Z",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 4.1.2: Select Dataset\n",
        "\n",
        "#@title 2： Prepare your protein sequences for inference\n",
        "\n",
        "#@markdown You have two options to provide your protein sequences:\n",
        "#@markdown - **Single Sequence: Enter a single SA sequence** into the input box, you can get a SA Sequence by clicking <a href=\"#get_SA_seq\">here</a>\n",
        "#@markdown - **Multiple Sequences: Select a dataset**\n",
        "\n",
        "mode = \"Single Sequence\" #@param ['Single Sequence', 'Multiple Sequences']\n",
        "\n",
        "##@markdown <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/InferenceFileFormat.png\" height=\"256\" align=\"center\" style=\"height:256px\">\n",
        "\n",
        "# seq = \"MdEpDvRvRvApEvKvSvCvEvQvAvCvElSvLvKvRlQvDvYlEvMvAlLlKvHvClTvEvAlLlLvSvLlGvQpYdSdMpAvDpFdTpGdPpCcPpLlEvIsEvRlIsKnIlElSsLlLlYsRnIlAlSsFcLlQsLvKvNvYvVvQvAnDvEvDsClRcHvVlLcGvEvGcLvAvKvGpEvDvAsFsRlAvVsLlClCvMcQvLvKvGvKnLlQvPsVsSlTvIsLlAvKvSsLvTpGdEpSdLpNdGpMdVdTcKpDsLsTvRvLvKnTvLsLnSvEvTsEnTvAvTsSvNvAvLvSvGpYdHpVdEdDpLpDpEpGvSvCvNpGpWdHdFdRaPaPaPfRfGqIdTqSdSlEvEpYaTfLaCpKvRcFcLvEvQpGvIdCdRlYfGpAsQqCfTnShAhHsSdQpEvEsLsAvEvWnQsKvRvYlAvSvRlLvIvKvLlKvQvQvNpEvNvKdQdLdSdGdSdYpMlEvTvLlIvEvKcWlMvNpSdLpScPnEqKqVaLeSdEqCdIdEpGqVkKdVkEdHkNpPdDdLlSeVaTeVfSfTaKqKwSdHkQdTkWmTkFiAkLiTkCgKaPqAaRfMdLfYfRkVkAaLkLnYqDlAqHqRlPvHfFkSaIwIdAwIkSwAkGdDdSpTpTdQiVdStQdEdVgPdEvNsChQrEmWdIgGdGdKdMdAdQpNdGpLdDrHiYmViYmKiViGmImAmFgNtTgEnIgFfGaTkFhRkQiTwIiVwFtDdFrGvLhEtPrVtLhMtQdRiVhMiIyDgAyAfSaTvEpDfLqElYqLqMpHpAaKvQpQvLvVvTpTlAdKdRdWdDdSpSvSqKfTaIeIdDeFaEpPpNdEpTdTdDpLvElKvSvLlLcIvRvYlQdIqPdLpSdApDvQvLlFpTdQpSlVcLvDdKlSdLqTdKqSvNsYpQlSsRnLvHsDsLlLlYsIlElEvIvAlQlYlKvElIqSsKvFfNfLdKwVwQkLkQwIkLaAqSkFdMfLdTdGdVqSfGtGdAmKdYgAdQdNdGqQkLiFkGiRkFtKwLtTpEaTfLqSaEpDqTaLpAsGsRvLsVcMvTfKfVfNfAkVkYkLkLfPfVdPdKpQdKpLdVpQdTdQpGpThKgEgKyViYyEiAwTtIwEpEdKaTwKrEtYmItFmLtRmLgShRnEvCrCcEvEvLvNvLdRdPhDgCdDiTtQiViEgLmQrFiQdLgNnRcLsPvLsCsElMlHsYvAlLsDvRlIdKpDdNcGcVlLlFvPpDdIlSvMdTqPfTpIqPpWqSpPpNvRnQpWqDdEpQlLqDdPpRpLqNdApKlQlKsElAlVlLsAlIqTqTrPdLlAvIgQqLdPhPaVfLeIeIaGePdYfGqTlGcKqTlFvTsLvAlQsAsVvKvHvIqLlQvQdQpEpTaRaIeLeIeCeTeHqSdNpSlAsAlDlLcYsIqKpDpYrLqHvPvYvVvEvAvGvNcPvQlAsRqPeLaReVdYdFdRfNpRpWqVlKqTlVhHdPpVsVvHvQvYrCfLqIaSdSpAvHpSrTtFgQdMqPdQeKlEvDnIsLvKvHhRrVyVyVgVyTyLlNvTvSlQlYsLvCvQvLrDpLdEaPqGcFsFhTlHeIyLeLyDeEqAqAlQaAdMfEqCsEsTvIsMsPnLvAsLsAyTdQsNsTrRhIhVyLyAyGhDhHvMlQaLhSfPdFdVgYsSnEpFsAsRvEsRsNvLrHrVgStLsLsDnRvLvYlEvHvYhPdApEpFrPsCsRhIrLyLrCaEeNgYqRfSaHdEqAvIlIqNcYlTcScEsLrFrYpEvGvKrLhMdAySpGhKpQfPdAaHqKpDpFaYaPqLqTfFeFaTeAqRpGfEaDwVdQaEdKpNtShTnAwFiYaNtNvAsElVlFvEvVvVlEvRvVlEvEvLcRvRvKrWpPdVcAsWvGpKpLdDdDlGlSlIaGeVeVeTePqYtAdDvQnVlFvRsIsRcAvEvLcRvKvKvRvLsSnDnVhNhVyEdRyVlLlNqVcQpGvKaQaFhRqVeLyFeLySeTqVrRdTdRpHvTvCqKdHdKpQpTaPvIdKdKpKpEdQsLnLdEpDcSdTnEpDrLgDcYsGqFqLlSaNdYsKnLsLvNsTsArIsTsRrAhQhShLhVyAyVyVyGyDhPlIlAsLsClSnIgGyRrCnRnKsFsWsElRvFsIqAvLvCcHqEvNvSvSrLyHpGdIdThFpEvQrIsKvAvQvLsEvAvLsEnLsKdKnTpYpVpLpNpPsLnAhPnEdFpIdPpRsAsLsRhLhQpHpSvGdSdTdNdKdQdQdQdSdPdPdKdGdKdSdLdHdHdTdQdNdDdHdFdQdNdDdGdIdVdQdPdNtPgShVdLdIdGhNdPdIdRdAdYdTdPdPdPdPdLdGdPdHdPdNdLdGdKdSdPdSdPpVpQdRdIdDdPpHdTdGdTdSdIdLdYdVdPpAdVdYdGdGdNdVdVdMdSdVdPdLaPdVdPdWdTdGfYdQdGdRdFdAdVdDdPdRdIdIdTdHdQdAdAdMdAtYdNdMdNdLdLdQdTdHdGdRdGdSdPdIdPdYdGdLdGdHdHdPdPdVdTdIdGdQdPdQdNdQdHdQdEdKdDdQdHdEdQdNdRdNdGdKdSdDdTdNdNdSdGdPdEdIdNdKdIdRdTdPdEdKdKdPdTdEdPdKdQdVdDdLdEdSdNdPdQdNdRdSdPdEdSdRdPdSdVdVdYdPdSdTdKdFdPdRdKyDdNdLdNdPwRfHdIdNdLdPdLdPpAdPpHpApQdYdAdIdPvNpRpHpFpHdPdLdPdQdLdPdRdPdPdFdPdIdPdQdQdHdTdLdLdNdQdQdQyNdNdLdPdEdQdPdNdQdIdPdPdQdPdNdQdVdVdQdQdQdSdQdLdNdQdQdPdQdQdPdPdPdQdLdSdPdAdYdQdAdGdPdNdNdAdFdFdNdSdAdVdAdHdRdPdQdSdPdPdAdEdAdVdIdPdEdQdQdPdPdPdMdLdQdEdGdHdSdPdLdRdAdIdAdQdPdGdPdIdLdPdSdHdLdNdSdFdIdDdEdNdPdSdGdLdPdIdGdEyAdLdDdRdIdHdGdSdVdAdLdEdTdLdRdQdQdQdAdRdFdQdQdWdSdEdHdHdAdFdLdSdQdGdSdAdPdYdPdHdHdHdHdPdHdLdQdHdLdPdQdPdPdLdGdLdHdQdPdPdVdRdAdDdWdKdLdTdSdSdAdEdDdEdVdEdTdTdYdSdRdFdQdDyLdIdRdEdLdSdHdRdDdQdSdEdTdRdEdLdAdEdMdPrPtPdQpSdRpLdLsQdYdRdQdVdQdSdRdSdPdPdAdVdPdSdPdPdSdSdTdDdHdSdSdHdFdSdNdFdNdDdNdSdRdDdIdEdVdAdSdNdPdAdFdPdQdRdLdPdPdQdIdFdNdSdPdFdSdLdPdSdEdHdLdAdPdPdPdLdKdYdLdAdPdDyGdAdWdTdFdAdNdLdQdQdNdHdLdMdGdPdGdFdPdYdGdLdPdPdLdPdHdRdPdPdQdNdPdFdVdQdIdQdNdHdQdHdAdIdGdQdEdPdFdHdPdLdSdSdRdTgVdSgSdSdSdLdPdSdLdEdEdYdEdPdRdGdPdGdRdPdLdYdQdRdRdIdSdSdSdSdVdQdPdCdSdEdEdVdSdTdPdQyDdSdLdAdQdCyKdEdLdQdDdHdSdNdQdSdSdFdNdFdSdSdPdEdSdWdVdNdTdTdSdSdTdPdYdQdNdIdPdCdNdGdSdSdRdTdAdQdPdRdEdLdIdAdPdPdKdTyVdKdPdPdEdDdQdLdKdSdEdNdLdEdVdSdSdSdFdNdYdSdVdLdQdHdLdGdQdFdPdPdLdMdPdNdKdQdIdAdEdSdAdNdSdSdSdPdQdSySdAdGdGdKdPdAdMdSdYdAdSdAdLdRdAdPdPdKdPdRdPdPdPdEdQdAdKdKdSdSdDdPpLpSpLpFpQpEpLpSdLdGdSdSdSdGdSdNdGdFdYdSsYpFgKd\" # @param {type:\"string\"}\n",
        "\n",
        "# print(font.RED+font.BOLD+f\"Data type: {data_type}\"+font.RESET)\n",
        "\n",
        "if mode == \"Multiple Sequences\":\n",
        "  dataset_dropdown = select_dataset()\n",
        "else:\n",
        "  input_seq = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Paste the Structure-Aware Sequence here',\n",
        "    description='SA Sequence:',\n",
        "    disabled=False)\n",
        "  input_seq.layout.width = '500px'\n",
        "  display(input_seq)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRMuOIe5DfLr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 4.1.3: Select Model\n",
        "\n",
        "#@markdown As we use Parameter-Efficient Fine-Tuning Technique, which allows us to store model weights into an small adapter without adjusting the original model weights during training, it's necessary to specify both the original model and adapter for prediction.\n",
        "#@markdown\n",
        "#@markdown 1. Select a **base model**\n",
        "#@markdown\n",
        "#@markdown 2. By running this cell, you will see an **adapter combobox**. We provide two ways to select your adapter:\n",
        "#@markdown  - Select a **local adapters** from the combobox.\n",
        "#@markdown   - Enter a **huggingface repository name** to the combobox. (e.g. \"SaProtAdapters/DeepLoc_cls10_35M\")\n",
        "#@markdown\n",
        "#@markdown You can also find some officical adapters in [here](https://huggingface.co/SaProtAdapters)\n",
        "model_path = \"westlake-repl/SaProt_35M_AF2\" #@param ['westlake-repl/SaProt_35M_AF2', 'westlake-repl/SaProt_650M_AF2'] {allow-input:true}\n",
        "\n",
        "use_adapter = True # @param {type:\"boolean\"}\n",
        "if use_adapter:\n",
        "  adapter_combobox = select_adapter()\n",
        "\n",
        "# print(font.RED+font.BOLD+f\"Adapter: {adapter_combobox.value}\"+font.RESET)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.1.4: Get your Result\n",
        "from transformers import EsmTokenizer\n",
        "import torch\n",
        "import copy\n",
        "import datetime\n",
        "import sys\n",
        "from saprot.model.esm.esm_classification_model import EsmClassificationModel\n",
        "from saprot.model.esm.esm_regression_model import EsmRegressionModel\n",
        "\n",
        "from saprot.scripts.training import my_load_model\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################# task config ##################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "# @markdown Click the run button to make prediction.\n",
        "\n",
        "# @markdown <font color=\"red\">**Note that:**</font> When predicting a category, the index of categories starts from zero.\n",
        "\n",
        "if use_adapter:\n",
        "  if adapter_combobox.value =='':\n",
        "    print(\"Please select an adatper!\")\n",
        "    sys.exit()\n",
        "\n",
        "  if \". \" in adapter_combobox.value:\n",
        "    adapter_path = ADAPTER_HOME / task_type / adapter_combobox.value.split('. ')[1]\n",
        "  else:\n",
        "    adapter_path = adapter_combobox.value\n",
        "################################################################################\n",
        "##################################### config ###################################\n",
        "################################################################################\n",
        "from saprot.config.config_dict import Default_config\n",
        "config = copy.deepcopy(Default_config)\n",
        "\n",
        "if task_type in [ \"classification\", \"token_classification\"]:\n",
        "  config.model.kwargs.num_labels = num_of_categories.value\n",
        "\n",
        "config.model.model_py_path = model_type_dict[task_type]\n",
        "# config.model.save_path = model_save_path\n",
        "config.model.kwargs.config_path = model_path\n",
        "if use_adapter:\n",
        "  config.model.kwargs.lora_config_path = adapter_path\n",
        "else:\n",
        "  config.model.kwargs.lora_config_path = None\n",
        "\n",
        "config.model.kwargs.use_lora = True\n",
        "config.model.kwargs.lora_inference = True\n",
        "\n",
        "################################################################################\n",
        "################################### inference ##################################\n",
        "################################################################################\n",
        "from peft import PeftModelForSequenceClassification\n",
        "\n",
        "model = my_load_model(config.model)\n",
        "tokenizer = EsmTokenizer.from_pretrained(config.model.kwargs.config_path)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "print(\"#\"*60)\n",
        "print(font.RED+font.BOLD+f\"Inference task type: {task_type}\"+font.RESET)\n",
        "if mode == \"Multiple Sequences\":\n",
        "  dataset_path = DATASET_HOME / f\"{dataset_dropdown.value.split('. ')[1]}.csv\"\n",
        "  print(font.RED+font.BOLD+f\"Dataset: {dataset_path}\"+font.RESET)\n",
        "else:\n",
        "  print(font.RED+font.BOLD+f\"Dataset: {input_seq.value}\"+font.RESET)\n",
        "print(font.RED+font.BOLD+f\"Model: {model_path}\"+font.RESET)\n",
        "if use_adapter:\n",
        "  print(font.RED+font.BOLD+f\"Adapter: {adapter_path}\"+font.RESET)\n",
        "\n",
        "outputs_list=[]\n",
        "\n",
        "if mode == \"Multiple Sequences\":\n",
        "  timestamp = str(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "  output_file = OUTPUT_HOME / f'output_{timestamp}.txt'\n",
        "  df = pd.read_csv(saseq_csv_path)\n",
        "  for index in tqdm(range(len(df))):\n",
        "    seq = df['Sequence'].iloc[index]\n",
        "    inputs = tokenizer(seq, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model(inputs)\n",
        "    outputs_list.append(outputs)\n",
        "else:\n",
        "  print(\"You are making inference based on a sequence that you entered\")\n",
        "  seq = input_seq.value\n",
        "  inputs = tokenizer(seq, return_tensors=\"pt\")\n",
        "  inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "  outputs = model(inputs)\n",
        "  outputs_list.append(outputs)\n",
        "\n",
        "################################################################################\n",
        "##################################### output ###################################\n",
        "################################################################################\n",
        "\n",
        "print()\n",
        "print(\"#\"*60)\n",
        "print(font.RED+font.BOLD+\"outputs:\"+font.RESET)\n",
        "\n",
        "if task_type == \"classification\":\n",
        "  import torch.nn.functional as F\n",
        "  softmax_output_list = [F.softmax(output, dim=1).squeeze().tolist() for output in outputs_list]\n",
        "  for index, output in enumerate(softmax_output_list):\n",
        "    print(f\"For Sequence {index}, Prediction: Category {output.index(max(output))}, Probability: {output}\")\n",
        "elif task_type == \"regression\":\n",
        "  output_list = [output.squeeze().tolist() for output in outputs_list]\n",
        "  for index, output in enumerate(outputs_list):\n",
        "    print(f\"For Sequence {index}, Prediction: Value {output.item()}\")\n",
        "elif task_type == \"token_classification\":\n",
        "  import torch.nn.functional as F\n",
        "  softmax_output_list = [F.softmax(output, dim=-1).squeeze().tolist() for output in outputs_list]\n",
        "  # print(softmax_output_list)\n",
        "  print(\"The probability of each category:\")\n",
        "  for seq_index, seq in enumerate(softmax_output_list):\n",
        "    seq_prob_df = pd.DataFrame(seq)\n",
        "    print('='*100)\n",
        "    print(f'Sequence {seq_index + 1}:')\n",
        "    print(seq_prob_df[1:-1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HjFkN55y1g4k",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2: Mutational Effect"
      ],
      "metadata": {
        "id": "7T1FZpi8_6FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.2.1: Sequences and Mutations\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "#@markdown ### 1. Input and Output\n",
        "\n",
        "#@markdown You have four different combinations of **mutation task** and **mode** to choose from:\n",
        "\n",
        "#@markdown |Combination| Input | Output |\n",
        "#@markdown | --- | --- | --- |\n",
        "#@markdown |`Single-site or Multi-site mutagenesis` + `Single Sequence`| Enter **a SA sequence** and **a mutation information**| a score of the mutation |\n",
        "#@markdown |`Single-site or Multi-site mutagenesis` + `Multiple Sequences`| Select **a dataset** and upload **a .csv file containing mutation information**| a .csv file containing the scores of mutations |\n",
        "#@markdown |`Saturation mutagenesis` + `Single Sequence`| Enter **a SA sequence**| a .csv file containing the scores of all mutation on every position of the sequence |\n",
        "#@markdown |`Saturation mutagenesis` + `Multiple Sequences`| Select **a dataset**| a .zip file containing the .csv files of the Saturation mutagenesis on every sequence |\n",
        "\n",
        "#@markdown <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/mutation_input_output.png\" height=\"500\" width=\"800px\" align=\"center\">\n",
        "\n",
        "mutation_task = \"Saturation mutagenesis\" #@param [\"Single-site or Multi-site mutagenesis\", \"Saturation mutagenesis\"]\n",
        "mode = \"Multiple Sequences\" #@param ['Single Sequence', 'Multiple Sequences']\n",
        "\n",
        "#@markdown You can obtain a single SA sequence from <a href=\"#get_SA_seq\">here</a>\n",
        "\n",
        "#@markdown Click the run button to provide your **Sequences and Mutations**\n",
        "\n",
        "\n",
        "##@markdown - **Single Sequence: Enter a single SA sequence** into the input box, you can get a SA Sequence by clicking <a href=\"#get_SA_seq\">here</a>\n",
        "##@markdown - **Multiple Sequences: Select a dataset** and then **upload a .csv file as mutations**\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### 2. Mutation Information of `Single-site or Multi-site mutagenesis`\n",
        "#@markdown Here is the detail about the representation of **mutation information**: <a name=\"mutation info\"></a>\n",
        "\n",
        "#@markdown | mode | mutation information|\n",
        "#@markdown | --- | --- |\n",
        "#@markdown | Single-site mutagenesis | H87Y |\n",
        "#@markdown | Multi-site mutagenesis | H87Y:V162M:P179L:P179R |\n",
        "\n",
        "#@markdown - For `Single-site mutagenesis`, we use a term like \"H87Y\" to denote the mutation, where the first letter represents the **original amino acid**, the number in the middle represents the **mutation site**, and the last letter represents the **mutated amino acid**,\n",
        "\n",
        "#@markdown - For `Multi-site mutagenesis`, we use a colon \":\" to connect each single-site mutations, such as \"H87Y:V162M:P179L:P179R\".\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### 3. Format of the uploaded .csv file containing mutation information\n",
        "\n",
        "#@markdown For Multiple Sequences, you are required to **upload an additional .csv file** as your mutation information.\n",
        "#@markdown <font color=red>Please ensure that each mutation in the mutation CSV file corresponds to each Sequence in the dataset CSV file.</font>\n",
        "\n",
        "#@markdown <img src=\"https://github.com/LUCKYDOGQAQ/ColabSaProt_dev/raw/main/MutationFormat.png\" height=\"256\" align=\"center\" style=\"height:256px\">\n",
        "\n",
        "\n",
        "# seq = \"MdEpDvRvRvApEvKvSvCvEvQvAvCvElSvLvKvRlQvDvYlEvMvAlLlKvHvClTvEvAlLlLvSvLlGvQpYdSdMpAvDpFdTpGdPpCcPpLlEvIsEvRlIsKnIlElSsLlLlYsRnIlAlSsFcLlQsLvKvNvYvVvQvAnDvEvDsClRcHvVlLcGvEvGcLvAvKvGpEvDvAsFsRlAvVsLlClCvMcQvLvKvGvKnLlQvPsVsSlTvIsLlAvKvSsLvTpGdEpSdLpNdGpMdVdTcKpDsLsTvRvLvKnTvLsLnSvEvTsEnTvAvTsSvNvAvLvSvGpYdHpVdEdDpLpDpEpGvSvCvNpGpWdHdFdRaPaPaPfRfGqIdTqSdSlEvEpYaTfLaCpKvRcFcLvEvQpGvIdCdRlYfGpAsQqCfTnShAhHsSdQpEvEsLsAvEvWnQsKvRvYlAvSvRlLvIvKvLlKvQvQvNpEvNvKdQdLdSdGdSdYpMlEvTvLlIvEvKcWlMvNpSdLpScPnEqKqVaLeSdEqCdIdEpGqVkKdVkEdHkNpPdDdLlSeVaTeVfSfTaKqKwSdHkQdTkWmTkFiAkLiTkCgKaPqAaRfMdLfYfRkVkAaLkLnYqDlAqHqRlPvHfFkSaIwIdAwIkSwAkGdDdSpTpTdQiVdStQdEdVgPdEvNsChQrEmWdIgGdGdKdMdAdQpNdGpLdDrHiYmViYmKiViGmImAmFgNtTgEnIgFfGaTkFhRkQiTwIiVwFtDdFrGvLhEtPrVtLhMtQdRiVhMiIyDgAyAfSaTvEpDfLqElYqLqMpHpAaKvQpQvLvVvTpTlAdKdRdWdDdSpSvSqKfTaIeIdDeFaEpPpNdEpTdTdDpLvElKvSvLlLcIvRvYlQdIqPdLpSdApDvQvLlFpTdQpSlVcLvDdKlSdLqTdKqSvNsYpQlSsRnLvHsDsLlLlYsIlElEvIvAlQlYlKvElIqSsKvFfNfLdKwVwQkLkQwIkLaAqSkFdMfLdTdGdVqSfGtGdAmKdYgAdQdNdGqQkLiFkGiRkFtKwLtTpEaTfLqSaEpDqTaLpAsGsRvLsVcMvTfKfVfNfAkVkYkLkLfPfVdPdKpQdKpLdVpQdTdQpGpThKgEgKyViYyEiAwTtIwEpEdKaTwKrEtYmItFmLtRmLgShRnEvCrCcEvEvLvNvLdRdPhDgCdDiTtQiViEgLmQrFiQdLgNnRcLsPvLsCsElMlHsYvAlLsDvRlIdKpDdNcGcVlLlFvPpDdIlSvMdTqPfTpIqPpWqSpPpNvRnQpWqDdEpQlLqDdPpRpLqNdApKlQlKsElAlVlLsAlIqTqTrPdLlAvIgQqLdPhPaVfLeIeIaGePdYfGqTlGcKqTlFvTsLvAlQsAsVvKvHvIqLlQvQdQpEpTaRaIeLeIeCeTeHqSdNpSlAsAlDlLcYsIqKpDpYrLqHvPvYvVvEvAvGvNcPvQlAsRqPeLaReVdYdFdRfNpRpWqVlKqTlVhHdPpVsVvHvQvYrCfLqIaSdSpAvHpSrTtFgQdMqPdQeKlEvDnIsLvKvHhRrVyVyVgVyTyLlNvTvSlQlYsLvCvQvLrDpLdEaPqGcFsFhTlHeIyLeLyDeEqAqAlQaAdMfEqCsEsTvIsMsPnLvAsLsAyTdQsNsTrRhIhVyLyAyGhDhHvMlQaLhSfPdFdVgYsSnEpFsAsRvEsRsNvLrHrVgStLsLsDnRvLvYlEvHvYhPdApEpFrPsCsRhIrLyLrCaEeNgYqRfSaHdEqAvIlIqNcYlTcScEsLrFrYpEvGvKrLhMdAySpGhKpQfPdAaHqKpDpFaYaPqLqTfFeFaTeAqRpGfEaDwVdQaEdKpNtShTnAwFiYaNtNvAsElVlFvEvVvVlEvRvVlEvEvLcRvRvKrWpPdVcAsWvGpKpLdDdDlGlSlIaGeVeVeTePqYtAdDvQnVlFvRsIsRcAvEvLcRvKvKvRvLsSnDnVhNhVyEdRyVlLlNqVcQpGvKaQaFhRqVeLyFeLySeTqVrRdTdRpHvTvCqKdHdKpQpTaPvIdKdKpKpEdQsLnLdEpDcSdTnEpDrLgDcYsGqFqLlSaNdYsKnLsLvNsTsArIsTsRrAhQhShLhVyAyVyVyGyDhPlIlAsLsClSnIgGyRrCnRnKsFsWsElRvFsIqAvLvCcHqEvNvSvSrLyHpGdIdThFpEvQrIsKvAvQvLsEvAvLsEnLsKdKnTpYpVpLpNpPsLnAhPnEdFpIdPpRsAsLsRhLhQpHpSvGdSdTdNdKdQdQdQdSdPdPdKdGdKdSdLdHdHdTdQdNdDdHdFdQdNdDdGdIdVdQdPdNtPgShVdLdIdGhNdPdIdRdAdYdTdPdPdPdPdLdGdPdHdPdNdLdGdKdSdPdSdPpVpQdRdIdDdPpHdTdGdTdSdIdLdYdVdPpAdVdYdGdGdNdVdVdMdSdVdPdLaPdVdPdWdTdGfYdQdGdRdFdAdVdDdPdRdIdIdTdHdQdAdAdMdAtYdNdMdNdLdLdQdTdHdGdRdGdSdPdIdPdYdGdLdGdHdHdPdPdVdTdIdGdQdPdQdNdQdHdQdEdKdDdQdHdEdQdNdRdNdGdKdSdDdTdNdNdSdGdPdEdIdNdKdIdRdTdPdEdKdKdPdTdEdPdKdQdVdDdLdEdSdNdPdQdNdRdSdPdEdSdRdPdSdVdVdYdPdSdTdKdFdPdRdKyDdNdLdNdPwRfHdIdNdLdPdLdPpAdPpHpApQdYdAdIdPvNpRpHpFpHdPdLdPdQdLdPdRdPdPdFdPdIdPdQdQdHdTdLdLdNdQdQdQyNdNdLdPdEdQdPdNdQdIdPdPdQdPdNdQdVdVdQdQdQdSdQdLdNdQdQdPdQdQdPdPdPdQdLdSdPdAdYdQdAdGdPdNdNdAdFdFdNdSdAdVdAdHdRdPdQdSdPdPdAdEdAdVdIdPdEdQdQdPdPdPdMdLdQdEdGdHdSdPdLdRdAdIdAdQdPdGdPdIdLdPdSdHdLdNdSdFdIdDdEdNdPdSdGdLdPdIdGdEyAdLdDdRdIdHdGdSdVdAdLdEdTdLdRdQdQdQdAdRdFdQdQdWdSdEdHdHdAdFdLdSdQdGdSdAdPdYdPdHdHdHdHdPdHdLdQdHdLdPdQdPdPdLdGdLdHdQdPdPdVdRdAdDdWdKdLdTdSdSdAdEdDdEdVdEdTdTdYdSdRdFdQdDyLdIdRdEdLdSdHdRdDdQdSdEdTdRdEdLdAdEdMdPrPtPdQpSdRpLdLsQdYdRdQdVdQdSdRdSdPdPdAdVdPdSdPdPdSdSdTdDdHdSdSdHdFdSdNdFdNdDdNdSdRdDdIdEdVdAdSdNdPdAdFdPdQdRdLdPdPdQdIdFdNdSdPdFdSdLdPdSdEdHdLdAdPdPdPdLdKdYdLdAdPdDyGdAdWdTdFdAdNdLdQdQdNdHdLdMdGdPdGdFdPdYdGdLdPdPdLdPdHdRdPdPdQdNdPdFdVdQdIdQdNdHdQdHdAdIdGdQdEdPdFdHdPdLdSdSdRdTgVdSgSdSdSdLdPdSdLdEdEdYdEdPdRdGdPdGdRdPdLdYdQdRdRdIdSdSdSdSdVdQdPdCdSdEdEdVdSdTdPdQyDdSdLdAdQdCyKdEdLdQdDdHdSdNdQdSdSdFdNdFdSdSdPdEdSdWdVdNdTdTdSdSdTdPdYdQdNdIdPdCdNdGdSdSdRdTdAdQdPdRdEdLdIdAdPdPdKdTyVdKdPdPdEdDdQdLdKdSdEdNdLdEdVdSdSdSdFdNdYdSdVdLdQdHdLdGdQdFdPdPdLdMdPdNdKdQdIdAdEdSdAdNdSdSdSdPdQdSySdAdGdGdKdPdAdMdSdYdAdSdAdLdRdAdPdPdKdPdRdPdPdPdEdQdAdKdKdSdSdDdPpLpSpLpFpQpEpLpSdLdGdSdSdSdGdSdNdGdFdYdSsYpFgKd\" # @param {type:\"string\"}\n",
        "\n",
        "# print(font.RED+font.BOLD+f\"Data type: {data_type}\"+font.RESET)\n",
        "\n",
        "if mode == \"Multiple Sequences\":\n",
        "  dataset_dropdown = select_dataset()\n",
        "  if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "    print(\"=\"*100)\n",
        "    print(font.RED+font.BOLD+\"please upload a .csv mutation file!\"+font.RESET)\n",
        "    upload_path = Path().resolve() / \"saprot\" / \"upload_files\"\n",
        "    mutation_csv_path = upload_file(upload_path)\n",
        "\n",
        "else:\n",
        "  input_seq = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Paste the Structure-Aware Sequence here',\n",
        "    # description='SA Sequence:',\n",
        "    disabled=False)\n",
        "  input_seq.layout.width = '500px'\n",
        "  print(font.RED+font.BOLD+\"Structure-Aware Sequence:\"+font.RESET)\n",
        "  display(input_seq)\n",
        "  if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "    input_mut = ipywidgets.Text(\n",
        "      value=None,\n",
        "      placeholder='Paste the Structure-Aware Sequence here',\n",
        "      # description='SA Sequence:',\n",
        "      disabled=False)\n",
        "    print(font.RED+font.BOLD+\"Mutation:\"+font.RESET)\n",
        "    input_mut.layout.width = '500px'\n",
        "    display(input_mut)\n",
        "\n"
      ],
      "metadata": {
        "id": "hk3WZxCSCLYo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.2.2: Get your Result\n",
        "\n",
        "################################################################################\n",
        "################################# Task Info ####################################\n",
        "################################################################################\n",
        "model_path = \"westlake-repl/SaProt_650M_AF2\"\n",
        "\n",
        "print(font.RED+font.BOLD)\n",
        "print(f\"Mutation task: {mutation_task}\")\n",
        "print(f\"Mode: {mode}\")\n",
        "print(f\"Model: {model_path}\")\n",
        "if mode == \"Multiple Sequences\":\n",
        "  dataset_path = DATASET_HOME / f\"{dataset_dropdown.value.split('. ')[1]}.csv\"\n",
        "  print(font.RED+font.BOLD+f\"Dataset: {dataset_path}\"+font.RESET)\n",
        "else:\n",
        "  print(font.RED+font.BOLD+f\"Dataset: {input_seq.value}\"+font.RESET)\n",
        "\n",
        "print(font.RESET)\n",
        "\n",
        "print(f\"Predicting...\")\n",
        "timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "\n",
        "################################################################################\n",
        "################################# load model ###################################\n",
        "################################################################################\n",
        "\n",
        "from saprot.model.esm.esm_foldseek_mutation_model import EsmFoldseekMutationModel\n",
        "\n",
        "config = {\n",
        "    \"foldseek_path\": None,\n",
        "    \"config_path\": model_path,\n",
        "    \"load_pretrained\": True,\n",
        "}\n",
        "model = EsmFoldseekMutationModel(**config)\n",
        "tokenizer = model.tokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### Single Sequence ####################################\n",
        "################################################################################\n",
        "if mode == \"Single Sequence\":\n",
        "\n",
        "  seq = input_seq.value\n",
        "  if mutation_task == \"Single-site or Multi-site mutagenesis\":\n",
        "    mut = input_mut.value\n",
        "    score = model.predict_mut(seq, mut)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(font.RED+font.BOLD+\"Output:\"+font.RESET)\n",
        "    print(f\"The score of mutation {mut} is {font.RED}{score}{font.RESET}\")\n",
        "\n",
        "  if mutation_task==\"Saturation mutagenesis\":\n",
        "    timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "    output_path = OUTPUT_HOME / f'{timestamp}_prediction_output.csv'\n",
        "\n",
        "    mut_dicts = []\n",
        "    for pos in range(1, int(len(seq) / 2)+1):\n",
        "      mut_dict = model.predict_pos_mut(seq, pos)\n",
        "      mut_dicts.append(mut_dict)\n",
        "\n",
        "    mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "    df = pd.DataFrame(mut_list)\n",
        "    df.to_csv(output_path, index=None)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(font.RED+font.BOLD+\"Output:\"+font.RESET)\n",
        "    files.download(output_path)\n",
        "    print(f\"\\n{font.RED+font.BOLD}The result has been saved to {output_path} and your local computer.{font.RESET}\")\n",
        "\n",
        "################################################################################\n",
        "########################### Multiple Sequences #################################\n",
        "################################################################################\n",
        "if mode == \"Multiple Sequences\":\n",
        "\n",
        "  dataset_path = DATASET_HOME / f\"{dataset_dropdown.value.split('. ')[1]}.csv\"\n",
        "  dataset_df = pd.read_csv(dataset_path)\n",
        "  results = []\n",
        "\n",
        "  if mutation_task==\"Single-site or Multi-site mutagenesis\":\n",
        "    # merge mutation info into dataset\n",
        "    mutation_df = pd.read_csv(mutation_csv_path)\n",
        "    assert(len(dataset_df) == len(mutation_df))\n",
        "    merged_df = pd.concat([dataset_df, mutation_df], axis=1)\n",
        "\n",
        "    for index, row in tqdm(merged_df.iterrows(), total=len(merged_df), leave=False, desc=f\"Predicting\"):\n",
        "     seq = row['Sequence']\n",
        "     mut_info = row['mutation']\n",
        "     results.append(model.predict_mut(seq, mut_info))\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(font.RED+font.BOLD+\"Output:\"+font.RESET)\n",
        "\n",
        "    result_df = pd.DataFrame()\n",
        "    result_df['Sequence'] = dataset_df['Sequence']\n",
        "    result_df['mutation'] = mutation_df['mutation']\n",
        "    result_df['score'] = results\n",
        "\n",
        "    output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_path).stem}.csv\"\n",
        "    result_df.to_csv(output_path, index=None)\n",
        "    files.download(output_path)\n",
        "    print(f\"{font.RED+font.BOLD}The result has been saved to {output_path} and your local computer {font.RESET}\")\n",
        "\n",
        "  else:\n",
        "    for index, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), leave=False, desc=f\"Predicting\"):\n",
        "      seq = row['Sequence']\n",
        "      mut_dicts = []\n",
        "      for pos in range(1, int(len(seq) / 2)+1):\n",
        "        mut_dict = model.predict_pos_mut(seq, pos)\n",
        "        mut_dicts.append(mut_dict)\n",
        "      mut_list = [{'mutation': key, 'score': value} for d in mut_dicts for key, value in d.items()]\n",
        "      result_df = pd.DataFrame(mut_list)\n",
        "      results.append(result_df)\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*100)\n",
        "    print(font.RED+font.BOLD+\"Output:\"+font.RESET)\n",
        "\n",
        "    zip_files = []\n",
        "    for i in range(len(results)):\n",
        "      output_path = OUTPUT_HOME / f\"{timestamp}_prediction_output_{Path(dataset_path).stem}_Sequence{i+1}.csv\"\n",
        "      results[i].to_csv(output_path, index=None)\n",
        "      zip_files.append(output_path)\n",
        "\n",
        "    # zip and download zip to local computer\n",
        "    zip_path = OUTPUT_HOME / f\"{timestamp}_{Path(dataset_path).stem}.zip\"\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "        for file in zip_files:\n",
        "            zipf.write(file, os.path.basename(file))\n",
        "    files.download(zip_path)\n",
        "    print(f\"{font.RED+font.BOLD}The result has been saved to {zip_path} and your local computer{font.RESET}\")"
      ],
      "metadata": {
        "id": "3zBHnqy3Ck0-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  5: (Optional) Get a Structure-Aware Sequence <a name=\"get_SA_seq\"></a>\n"
      ],
      "metadata": {
        "id": "bxJtRMY82f0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTLeeXglfzQo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 5.1: Input\n",
        "# @markdown You can obtain a Structure-Aware Sequence by providing your sequence in one of three data types:\n",
        "# @markdown - `Amino Acid Sequence` : Enter the Amino Acid Sequence into the box.\n",
        "# @markdown - `UniProt ID` : Enter the UniProt ID into the box.\n",
        "# @markdown - `PDB/CIF file` : Click the run button to upload a .pdb/.cif file.\n",
        "\n",
        "data_type = \"UniProt ID\" # @param [\"Amino Acid Sequence\", \"UniProt ID\", \"PDB/CIF file\"]\n",
        "\n",
        "if data_type == \"Amino Acid Sequence\":\n",
        "  raw_data = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Paste the Amino Acid Sequence here',\n",
        "    # description='AA Sequence:',\n",
        "    disabled=False)\n",
        "  raw_data.layout.width = '500px'\n",
        "  print(font.RED + font.BOLD + 'Amino Acid Sequence:'+ font.RESET)\n",
        "  display(raw_data)\n",
        "  # sa_seq = get_single_SASequence_by_data_type(data_type, raw_data.value)\n",
        "\n",
        "elif data_type == \"UniProt ID\":\n",
        "  raw_data = ipywidgets.Text(\n",
        "    value=None,\n",
        "    placeholder='Paste the UniProt ID here',\n",
        "    # description='UniProt ID:',\n",
        "    disabled=False)\n",
        "  raw_data.layout.width = '500px'\n",
        "  print(font.RED + font.BOLD + 'UniProt ID:'+ font.RESET)\n",
        "  display(raw_data)\n",
        "  # sa_seq = get_single_SASequence_by_data_type(data_type, raw_data.value)\n",
        "\n",
        "elif data_type == \"PDB/CIF file\":\n",
        "  # print(\"Please Upload a .pdb/.cif file at 2.\")\n",
        "  # sa_seq = get_single_SASequence_by_data_type(data_type, None)\n",
        "\n",
        "  # upload and unzip PDB file\n",
        "  print(font.RED+font.BOLD+\"Please upload a .pdb/.cif file\"+font.RESET)\n",
        "  pdb_file_path = upload_file(STRUCTURE_HOME)\n",
        "  print(\"=\"*100)\n",
        "  print(font.RED+font.BOLD+\"Successfully upload your .pdb/.cif file!\"+font.RESET)\n",
        "\n",
        "  raw_data = EasyDict({})\n",
        "  raw_data.value = [Path(pdb_file_path).stem]\n",
        "\n",
        "# UniProt_ID = \"P42694\" # @param {type:\"string\"}\n",
        "\n",
        "# print(sa_seq)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.2: Output\n",
        "#@markdown Click the run button to get the SA Sequence.\n",
        "sa_seq = get_single_SASequence_by_data_type(data_type, raw_data.value)\n",
        "\n",
        "print()\n",
        "print(\"=\"*100)\n",
        "print(f\"Amino Acid Sequence: {sa_seq[0::2]}\")\n",
        "print(f\"Structure Sequence: {sa_seq[1::2]}\")\n",
        "# print(f\"Structure-Aware Sequence: {sa_seq}\")\n",
        "print(\"=\"*100)\n",
        "print(font.RED + font.BOLD + \"The Structure-Aware Sequence is here, double click to select and copy it:\" + font.RESET)\n",
        "print(sa_seq)\n"
      ],
      "metadata": {
        "id": "Wf9lgkxtxqn4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6: (Optional) Example tasks"
      ],
      "metadata": {
        "id": "nKmdEKvb2-rL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC4NK-P0re7b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 6.1: Run example tasks directly\n",
        "################################################################################\n",
        "#               Advanced Config                #\n",
        "################################################################################\n",
        "\n",
        "batch_size = 2\n",
        "num_workers = 2\n",
        "max_epochs = 4\n",
        "\n",
        "limit_train_batches=10\n",
        "limit_val_batches=2\n",
        "limit_test_batches=100\n",
        "val_check_interval=0.2\n",
        "\n",
        "use_lora = True\n",
        "if use_lora == True:\n",
        "  init_lr = 1.0e-3\n",
        "else:\n",
        "  init_lr = 1.0e-5\n",
        "\n",
        "logger = False\n",
        "WANDB_API_KEY = \"\"\n",
        "\n",
        "model_path = \"westlake-repl/SaProt_35M_AF2\" # @param [\"westlake-repl/SaProt_35M_AF2\", \"westlake-repl/SaProt_650M_AF2\"]\n",
        "\n",
        "################################################################################\n",
        "#@title **Task Config**\n",
        "from pathlib import Path\n",
        "from easydict import EasyDict\n",
        "import copy\n",
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "import time\n",
        "\n",
        "#@markdown ### **Select an example task and Click the button**\n",
        "task_name = \"Thermostability\"  #@param ['DeepLoc_cls2', 'DeepLoc_cls10', 'EC', 'GO_BP', 'GO_CC', 'GO_MF', 'MetalIonBinding', 'Thermostability']\n",
        "example_tasks = ['DeepLoc', 'EC', 'GO', 'HumanPPI', 'MetalIonBinding', 'Thermostability', 'ClinVar', 'ProteinGym']\n",
        "model_save_path = Path(f\"/content/saprot/weights/{task_name}\")\n",
        "\n",
        "################################################################################\n",
        "#                  Dataset                 #\n",
        "################################################################################\n",
        "download_dataset(task_name)\n",
        "\n",
        "################################################################################\n",
        "#                 Config                   #\n",
        "################################################################################\n",
        "cmd = f\"from saprot.config.config_dict import {task_name}_config\"\n",
        "exec(cmd)\n",
        "config = copy.deepcopy(locals()[f\"{task_name}_config\"])\n",
        "\n",
        "config.model.save_path = model_save_path\n",
        "\n",
        "config.Trainer.accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "################################################################################\n",
        "\n",
        "config.dataset.kwargs.tokenizer = model_path\n",
        "config.model.kwargs.config_path = model_path\n",
        "\n",
        "config.Trainer.max_epochs = max_epochs\n",
        "config.dataset.dataloader_kwargs.batch_size = batch_size\n",
        "config.dataset.dataloader_kwargs.num_workers = num_workers\n",
        "\n",
        "config.model.kwargs.use_lora = use_lora\n",
        "config.model.lr_scheduler_kwargs.init_lr = init_lr\n",
        "\n",
        "config.Trainer.limit_train_batches=limit_train_batches\n",
        "config.Trainer.limit_val_batches=limit_val_batches\n",
        "config.Trainer.limit_test_batches=limit_test_batches\n",
        "config.Trainer.val_check_interval=val_check_interval\n",
        "\n",
        "# wandb config\n",
        "config.Trainer.logger = logger\n",
        "config.setting.os_environ.WANDB_API_KEY = WANDB_API_KEY\n",
        "\n",
        "################################################################################\n",
        "#               Run the task                 #\n",
        "################################################################################\n",
        "start_time = time.time()\n",
        "\n",
        "# if task_name in ['ClinVar', 'ProteinGym']:\n",
        "#   config.model.kwargs.use_lora = None\n",
        "#   if task_name != 'ClinVar':\n",
        "#     from saprot.scripts.mutation_zeroshot import my_zeroshot\n",
        "#     my_zeroshot(config)\n",
        "#   end_time = time.time()\n",
        "# else:\n",
        "#   # download_adapter_to_your_computer = True # @param {type:\"boolean\"}\n",
        "#   download_adapter_to_your_computer = True\n",
        "\n",
        "#   from saprot.scripts.training import finetune\n",
        "\n",
        "#   # config.model.kwargs.lora_config_path = \"/content/saprot/weights/DeepLoc_cls2\"\n",
        "#   config.model.kwargs.lora_config_path = None\n",
        "\n",
        "#   finetune(config)\n",
        "#   end_time = time.time()\n",
        "\n",
        "download_adapter_to_your_computer = True\n",
        "\n",
        "# config.model.kwargs.lora_config_path = \"/content/saprot/weights/DeepLoc_cls2\n",
        "config.model.kwargs.lora_config_path = None\n",
        "\n",
        "from saprot.scripts.training import finetune\n",
        "finetune(config)\n",
        "end_time = time.time()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#              save the adapter                #\n",
        "################################################################################\n",
        "if use_lora is True:\n",
        "  print(f\"Adapter is saved to \\\"{config.model.save_path}\\\"\")\n",
        "  if download_adapter_to_your_computer:\n",
        "    adapter_zip = config.model.save_path / f\"{task_name}.adapter.zip\"\n",
        "    !cd $config.model.save_path && zip -r $adapter_zip \"adapter_config.json\" \"adapter_model.safetensors\" \"README.md\"\n",
        "\n",
        "    files.download(adapter_zip)\n",
        "\n",
        "execution_time = end_time - start_time\n",
        "print(\"Training time: \", execution_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7: (Optional) Data Preparation"
      ],
      "metadata": {
        "id": "lnMe4AqSO2h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7.1: From `.fasta` to `.csv`\n",
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "\n",
        "aa_seq_dict = { \"Sequence\": [],\n",
        "                # \"label\": [],\n",
        "                # \"stage\":[]\n",
        "                }\n",
        "\n",
        "fa_file_path = upload_file(Path(\"/content/saprot/upload_files\"))\n",
        "with fa_file_path.open(\"r\") as fa:\n",
        "  for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "      aa_seq_dict[\"Sequence\"].append(str(record.seq))\n",
        "\n",
        "fa_df = pd.DataFrame(aa_seq_dict)\n",
        "print(fa_df[5:])\n",
        "\n",
        "csv_file_path = f'/content/saprot/upload_files/{fa_file_path.stem}.csv'\n",
        "fa_df.to_csv(csv_file_path, index=None)\n",
        "files.download(csv_file_path)\n",
        "\n",
        "################################################################################\n",
        "############################ .fa 2 .csv and split ##############################\n",
        "################################################################################\n",
        "\n",
        "# automatically_split_dataset = False # @param {type:\"boolean\"}\n",
        "# split = ['train', 'valid', 'test']\n",
        "\n",
        "# aa_seq_dict = { \"Sequence\": [],\n",
        "#                 \"label\": [],\n",
        "#                 \"stage\":[]}\n",
        "\n",
        "\n",
        "\n",
        "# if automatically_split_dataset:\n",
        "\n",
        "#   fa_file_path = upload_file(Path(\"/content/saprot/upload_files\"))\n",
        "#   label = fa_file_path.stem\n",
        "\n",
        "#   with fa_file_path.open(\"r\") as fa:\n",
        "#       for record in tqdm(SeqIO.parse(fa, 'fasta'), leave=True):\n",
        "#           aa_seq_dict[\"Sequence\"].append(str(record.seq))\n",
        "#           aa_seq_dict[\"label\"].append(label)\n",
        "#   weights = [0.8, 0.1, 0.1]\n",
        "#   aa_seq_dict[\"stage\"] = np.random.choice(split, size=len(aa_seq_dict[\"Sequence\"]), p=weights).tolist()\n",
        "\n",
        "# else:\n",
        "#   for i in range(3):\n",
        "#     print(font.RED+font.BOLD+f\"Please upload a .fa file as your {split[i]} dataset\")\n",
        "#     fa_file_path = upload_file(Path(\"/content/saprot/upload_files\"))\n",
        "#     label = fa_file_path.stem\n",
        "\n",
        "#     with fa_file_path.open(\"r\") as fa:\n",
        "#         for record in tqdm(SeqIO.parse(fa, 'fasta')):\n",
        "#             aa_seq_dict[\"Sequence\"].append(str(record.seq))\n",
        "#             aa_seq_dict[\"label\"].append(label)\n",
        "#             aa_seq_dict[\"stage\"].append(split[i])\n",
        "\n",
        "#     print()\n",
        "#     print(\"=\"*100)\n",
        "\n",
        "# fa_df = pd.DataFrame(aa_seq_dict)\n",
        "# timestamp = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
        "# fa_df.to_csv(f'/content/saprot/upload_files/{timestamp}.csv', index=None)\n",
        "# files.download(f'/content/saprot/upload_files/{timestamp}.csv')\n",
        "# print(fa_df[5:])"
      ],
      "metadata": {
        "id": "ubap7XoNVsb8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nKmdEKvb2-rL"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}